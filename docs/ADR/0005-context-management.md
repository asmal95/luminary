# ADR-0005: Управление контекстом LLM

## Статус
Принято

## Контекст
Файлы могут быть очень большими и превышать лимиты контекста LLM. Необходимо определить стратегию работы с большими файлами.

## Решение

### Стратегия для MVP:
- **По умолчанию**: Отправляем весь файл + изменения в одном запросе
- **Лимит контекста**: Настраиваемый параметр (по умолчанию без жесткого лимита)
- **Для больших файлов**: Если файл превышает лимит, применяется chunking с перекрытием

### Chunking стратегия (если потребуется):
- Разбиение файла на чанки с перекрытием (overlap) для сохранения контекста
- Анализ каждого чанка отдельно
- Агрегация комментариев из всех чанков

### Альтернативные стратегии (для будущего):
- Summary-first: сначала summary файла, потом детальный анализ изменений
- Пропуск с предупреждением для файлов выше лимита

### Конфигурация:
- `max_context_tokens`: максимальный размер контекста (опционально)
- `chunk_overlap_size`: размер перекрытия при chunking (по умолчанию 200 строк)

## Последствия

### Положительные:
- Работа с файлами любого размера
- Сохранение контекста при chunking
- Гибкость в настройке лимитов

### Отрицательные:
- Chunking может потерять глобальный контекст
- Увеличение количества запросов для больших файлов

### Будущие улучшения:
- Интеллектуальный chunking (по функциям/классам)
- Кэширование анализа неизмененных частей файла
